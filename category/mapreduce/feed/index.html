<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:wfw="http://wellformedweb.org/CommentAPI/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:atom="http://www.w3.org/2005/Atom"
 xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:slash="http://purl.org/rss/1.0/modules/slash/" 
 xmlns:georss="http://www.georss.org/georss"
 xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
 > <channel><title>Mapreduce &#8211; Wonderful Tech</title> <atom:link href="http://www.showsql.com/category/mapreduce/feed/" rel="self" type="application/rss+xml" /><link>https://www.showsql.com/</link> <description>Learning Always</description> <lastBuildDate>Fri, 30 Aug 2019 03:34:02 +0000</lastBuildDate> <language>en-US</language> <sy:updatePeriod> hourly </sy:updatePeriod> <sy:updateFrequency> 1 </sy:updateFrequency> <generator>https://wordpress.org/?v=5.5.1</generator> <image> <url>http://www.showsql.com/wp-content/uploads/2019/09/cropped-logo-1-32x32.jpg</url><title>Mapreduce &#8211; Wonderful Tech</title><link>https://www.showsql.com/</link> <width>32</width> <height>32</height> </image> <site xmlns="com-wordpress:feed-additions:1">168814421</site> <item><title>linear regression 线性回归</title><link>http://www.showsql.com/2019/08/06/linear-regression/</link> <comments>http://www.showsql.com/2019/08/06/linear-regression/#respond</comments> <dc:creator><![CDATA[jasonpeng]]></dc:creator> <pubDate>Tue, 06 Aug 2019 02:45:38 +0000</pubDate> <category><![CDATA[Mapreduce]]></category> <category><![CDATA[streaming]]></category> <guid isPermaLink="false">http://showsql.com/?p=1252</guid> <description><![CDATA[<p>在已知自变量和因变量的观察数据的情况下，用线性回归求解自变量和因变量之间的线性方程。 线性回归易于理解和编程。非常适合用来练手。 单自变量线性回归 已知自变量\(x\)和因变量\(y\)的观察数据： 41 90 42 93 43 98 20 64 25 78 40 71 58 88 60 86 假设两者之间满足公式: $$ y = ax + b $$ 由于每对观测数据都满足上面的公式。 共有\(N\)组数据，可得出: \begin{align} y_1 &#38; = ax_1 + b \\ y_2 &#38; = ax_2 + b \\ &#38; \dots \\ y_N &#38; = ax_N + b \end{align} [&#8230;]</p><p>The post <a href="http://www.showsql.com/2019/08/06/linear-regression/" target="_blank">linear regression 线性回归</a> first appeared on <a href="http://www.showsql.com/" target="_blank">Wonderful Tech</a>.</p>]]></description> <wfw:commentRss>http://www.showsql.com/2019/08/06/linear-regression/feed/</wfw:commentRss> <slash:comments>0</slash:comments> <post-id xmlns="com-wordpress:feed-additions:1">1252</post-id> </item> <item><title>find all triangles in the graph 发现图中所有的三角形</title><link>http://www.showsql.com/2019/08/03/find-all-triangles-in-the-graph/</link> <comments>http://www.showsql.com/2019/08/03/find-all-triangles-in-the-graph/#respond</comments> <dc:creator><![CDATA[jasonpeng]]></dc:creator> <pubDate>Sat, 03 Aug 2019 11:06:46 +0000</pubDate> <category><![CDATA[Mapreduce]]></category> <category><![CDATA[streaming]]></category> <guid isPermaLink="false">http://showsql.com/?p=1218</guid> <description><![CDATA[<p>一个有趣的问题。现有一张无向图。要统计出该图中所有的三角形。乍一看，用mapreduce框架解决，此问题无思路。 三角形一定有三条边，三个顶点构成。设顶点按顺时针依次为a，b，c。 此时，就有三条边，a-b,b-c,c-a。 假设现在不知道，a，b，c三点能形成三角形。但是知道存在边a-b和a-c。 那么只要b和c直接相连形成边，就能断定a，b，c能形成三角形。 以此为突破口，设计如下的算法流程： 三角形形成算法 发射边： 无向图中每个顶点关联的所有边都发射出去。发射(a,b)。 相同起点的边，收集起来。(a,b1),(a,b2),...,(a,bn)。 那么可知，b1,b2,...,bn之间，肯定通过顶点a相关联。因此， b1,b2,...,bn之间可能形成边。但是不确定，所以发射(bi,bj,a)。只要之后碰到(bi,bj)的直接边就能形成三角形。而已知的直接边，发射成(a,bi,-)。'-'相当于直接边的标记。 组合三角形 有两个相同点的边，收集起来。(a,b,-),(a,b,c1), (a,b,c2), ..., (a,b,cn)。 既有直接边，又有非直接边，的情形才能形成三角形。即，-,c1,c2,...,cn的形式。'-'一定要有，ci至少有一个。 发射三角行。符合条件2，就可形成三角形:(a,b,c1), (a,b,c2), ..., (a,b,cn)。 三角形去重 去重较简单。对三角形的三个顶点排序。借助mapreduce框架的排序功能。然后去重。 设计mapreduce任务 为上面的三个阶段，每个都设计一个mr任务。 发射边： map.py reduce.py 输出结果： 1,2,- 2,5,- 2,4,- 2,3,- 2,1,- 5,4,2 5,3,2 5,1,2 4,3,2 4,1,2 3,1,2 3,2,- 3,4,- 2,4,3 4,5,- 4,3,- 4,2,- 5,3,4 5,2,4 3,2,4 5,2,- 5,4,- 2,4,5 组合三角形： map2.py [&#8230;]</p><p>The post <a href="http://www.showsql.com/2019/08/03/find-all-triangles-in-the-graph/" target="_blank">find all triangles in the graph 发现图中所有的三角形</a> first appeared on <a href="http://www.showsql.com/" target="_blank">Wonderful Tech</a>.</p>]]></description> <wfw:commentRss>http://www.showsql.com/2019/08/03/find-all-triangles-in-the-graph/feed/</wfw:commentRss> <slash:comments>0</slash:comments> <post-id xmlns="com-wordpress:feed-additions:1">1218</post-id> </item> <item><title>naive Bayes classifier 朴素贝叶斯</title><link>http://www.showsql.com/2019/08/03/naive-bayes-classifier/</link> <comments>http://www.showsql.com/2019/08/03/naive-bayes-classifier/#respond</comments> <dc:creator><![CDATA[jasonpeng]]></dc:creator> <pubDate>Sat, 03 Aug 2019 07:36:32 +0000</pubDate> <category><![CDATA[Mapreduce]]></category> <category><![CDATA[streaming]]></category> <guid isPermaLink="false">http://showsql.com/?p=1183</guid> <description><![CDATA[<p>朴素贝叶斯是一种线性分类器。应用之前，要假设属性之间是独立的。先要在训练数据集上得出分类器。然后用分类器分类新数据。 符号类的数据集方便使用贝叶斯分类器。 符号训练数据的贝叶斯分类 数据格式：天气，温度，湿度，风力，是否可以打网球 sunny,hot,high,weak,no sunny,hot,high,strong,no overcast,hot,high,weak,yes rain,mild,high,weak,yes rain,cool,normal,weak,yes rain,cool,normal,strong,no overcast,cool,normal,strong,yes sunny,mild,high,weak,no sunny,cool,normal,weak,yes rain,mild,normal,weak,yes sunny,mild,normal,strong,yes overcast,mild,high,strong,yes overcast,hot,normal,weak,yes rain,mild,high,strong,no 从训练数据中，可以得出哪些值的概率了？ 易知道，有两大类：yes和no。可以求出P(yes)和P(no)。 在yes和no的两大类下，又可以确定这些属性的概率：*位置处为yes或者no。 P(cool &#124; *),P(high &#124; *) ,P(hot &#124; *) ,P(mild &#124; *) ,P(normal &#124; *) ,P(overcast &#124; *) ,P(rain &#124; *) ,P(strong &#124; *) ,P(sunny &#124; *) ,P(weak &#124; *) 举例说明如何计算这些值。 计算P(yes)。 统计yes类和no类的总个数。P(yes)就是yes类个数的百分比。 计算P(strong &#124; [&#8230;]</p><p>The post <a href="http://www.showsql.com/2019/08/03/naive-bayes-classifier/" target="_blank">naive Bayes classifier 朴素贝叶斯</a> first appeared on <a href="http://www.showsql.com/" target="_blank">Wonderful Tech</a>.</p>]]></description> <wfw:commentRss>http://www.showsql.com/2019/08/03/naive-bayes-classifier/feed/</wfw:commentRss> <slash:comments>0</slash:comments> <post-id xmlns="com-wordpress:feed-additions:1">1183</post-id> </item> <item><title>knn k近邻</title><link>http://www.showsql.com/2019/08/02/knn/</link> <comments>http://www.showsql.com/2019/08/02/knn/#respond</comments> <dc:creator><![CDATA[jasonpeng]]></dc:creator> <pubDate>Fri, 02 Aug 2019 11:00:42 +0000</pubDate> <category><![CDATA[Mapreduce]]></category> <category><![CDATA[streaming]]></category> <guid isPermaLink="false">http://showsql.com/?p=1146</guid> <description><![CDATA[<p>已经知道某些数据已经分类，现在对新数据进行分类。用什么方法对新数据分类了？ 看它的k个相距最近的邻居都属于哪些类。这k个最近邻中，所属类中，个数最多的类就选择为新数据的类。 算法的过程为： 计算新数据与已知数据集中每个数据的距离； 选出k个相距最小距离的邻居，这些邻居都在已知数据集中； 对这些邻居所属的类别计数； 计数最大的类别，就是结果 注意：knn不对已知数据集进行修改。 输入数据 已知数据：id;类别;位置 100;c1;1.0,1.0 101;c1;1.1,1.2 102;c1;1.2,1.0 103;c1;1.6,1.5 104;c1;1.3,1.7 105;c1;2.0,2.1 106;c1;2.0,2.2 107;c1;2.3,2.3 208;c2;9.0,9.0 209;c2;9.1,9.2 210;c2;9.2,9.0 211;c2;10.6,10.5 212;c2;10.3,10.7 213;c2;9.6,9.1 214;c2;9.4,10.4 215;c2;10.3,10.3 300;c3;10.0,1.0 301;c3;10.1,1.2 302;c3;10.2,1.0 303;c3;10.6,1.5 304;c3;10.3,1.7 305;c3;1.0,2.1 306;c3;10.0,2.2 307;c3;10.3,2.3 新数据：id;位置 1000;3.0,3.0 1001;10.1,3.2 1003;2.7,2.7 1004;5.0,5.0 1005;13.1,2.2 1006;12.7,12.7 设计mapreduce任务 map.py reduce.py 提交脚本 输出结果： 1000 c1 1001 c3 1003 c1 1004 c1 1005 [&#8230;]</p><p>The post <a href="http://www.showsql.com/2019/08/02/knn/" target="_blank">knn k近邻</a> first appeared on <a href="http://www.showsql.com/" target="_blank">Wonderful Tech</a>.</p>]]></description> <wfw:commentRss>http://www.showsql.com/2019/08/02/knn/feed/</wfw:commentRss> <slash:comments>0</slash:comments> <post-id xmlns="com-wordpress:feed-additions:1">1146</post-id> </item> <item><title>kmeans k均值聚类</title><link>http://www.showsql.com/2019/08/01/kmeans/</link> <comments>http://www.showsql.com/2019/08/01/kmeans/#respond</comments> <dc:creator><![CDATA[jasonpeng]]></dc:creator> <pubDate>Thu, 01 Aug 2019 12:16:34 +0000</pubDate> <category><![CDATA[Mapreduce]]></category> <category><![CDATA[streaming]]></category> <guid isPermaLink="false">http://showsql.com/?p=1134</guid> <description><![CDATA[<p>kmeans原理不难理解。本文着重讲述kmeans的算法编写过程。 算法框架： 初始化质心； 执行mapreduce任务：为每个点选最近的质心。统计每个质心所含的点，并计算新质心； 判断新质心与旧质心之间的距离是否小于某个阈值。如果小于某个阈值，则算法结束； 判断迭代次数是否超过最大迭代次数。如果没有，则算法结束，否则再次执行1，2，3，4。 输入点数据： 1 2 1 3 1 4 2 5 2 6 2 7 2 8 3 100 3 101 3 102 3 103 3 104 初始质心：第一行表示质心的数量，就是k值。之后行表示各个质心。各个质心要不同。 3 1.0 2.0 1.0 3.0 1.0 4.0 map.py 加载质心。对每个点选择距离最近的质心。发射&#60;质心，点>。 reduce.py 聚合每个质心的所有点，算出新质心。发射&#60;质心，新质心>。 判断终止： 两个终止条件，其中一个是判断质心变化是否小于某个阈值。这放在change.py中，同时在change.py中还要聚合更换新质心。 change.py 最大迭代次数的终止条件放在提交脚本里。 提交脚本： Hits: 10</p><p>The post <a href="http://www.showsql.com/2019/08/01/kmeans/" target="_blank">kmeans k均值聚类</a> first appeared on <a href="http://www.showsql.com/" target="_blank">Wonderful Tech</a>.</p>]]></description> <wfw:commentRss>http://www.showsql.com/2019/08/01/kmeans/feed/</wfw:commentRss> <slash:comments>0</slash:comments> <post-id xmlns="com-wordpress:feed-additions:1">1134</post-id> </item> </channel> </rss>